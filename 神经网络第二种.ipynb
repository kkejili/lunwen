{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a245556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33287\\anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 651, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 649, 16)           64        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 649, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 649, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 649, 16)           784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 649, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 649, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 324, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 324, 24)           1944      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 324, 24)           96        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 324, 24)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 324, 24)           2904      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 324, 24)           96        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 324, 24)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 162, 24)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 162, 32)           2336      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 162, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 162, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 162, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 162, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 162, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 81, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 81, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 81, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 81, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 81, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 81, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 81, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 40, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               327808    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 368,994\n",
      "Trainable params: 368,450\n",
      "Non-trainable params: 544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,BatchNormalization,ReLU,Flatten,Dropout,GlobalAveragePooling1D,Dense,Activation\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn import linear_model # 导入线性模型包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors # 导入KNN包\n",
    "import sys\n",
    "import natsort\n",
    "import random\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "#from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\无缺陷\")\n",
    "wu1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "zong = []\n",
    "label = []\n",
    "pic_files = [fn for fn in wu1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first = []\n",
    "wu = []\n",
    "s = 0\n",
    "for fn in pic_files:\n",
    "    df=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df1 = df[\"Untitled\"]\n",
    "    s+=1\n",
    "    wu.append(df1)\n",
    "    if s>100:\n",
    "        zong.append(abs(df1))\n",
    "    else:\n",
    "        zong.append(abs(df1))\n",
    "    first.append(0)\n",
    "    label.append(0)\n",
    "os.chdir(\"../\")\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\有缺陷\")\n",
    "you1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "pic_files = [fn for fn in you1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first1 = []\n",
    "you=[]\n",
    "for fn in pic_files:\n",
    "    dfy=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df2 = dfy[\"Untitled\"]\n",
    "    you.append(df2)\n",
    "    zong.append(abs(df2))\n",
    "    first1.append(1)\n",
    "    label.append(1)\n",
    "os.chdir(\"../\")\n",
    "print(label)\n",
    "print(len(label))\n",
    "\n",
    "x = zong\n",
    "y = label\n",
    "\n",
    "seed=66\n",
    "random.seed(seed)\n",
    "random.shuffle(x)\n",
    "random.seed(seed)#一定得重复在写一遍,和上面的seed要相同,不然y_batch和x_batch打乱顺序会不一样\n",
    "random.shuffle(y)\n",
    "\n",
    "#df1=pd.read_csv('自制0mm缺陷总.csv',sep=\",\",header=[0])\n",
    "xy = tf.convert_to_tensor(x)\n",
    "yy = tf.convert_to_tensor(y)\n",
    "yy = tf.one_hot(yy,2)\n",
    "#xy = tf.one_hot(xy,2)\n",
    "#print(y)\n",
    "#index = [i for i in range(len(y))]\n",
    "\n",
    "#np.random.shuffle(index)\n",
    "#x_train= xy[index]\n",
    "\n",
    "#y_train = y[index]\n",
    "\n",
    "#x1 = df1['1'].values.reshape(1,-1)\n",
    "#x1 = tf.convert_to_tensor(x1)\n",
    "#x1 = pd.Series(x1)\n",
    "#y = pd.Series(y)\n",
    "# y=tf.expand_dims(y,1)\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "# 特征数目\n",
    "TIME_PERIODS = 651\n",
    "# classnum=2为类别数，必须是2，因为你有两个类0,1\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    # 输入尺寸大小651,1，每次输入1个包含651特征的样本\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "    # 一维卷积，16个大小为3的卷积核，步长为1,\n",
    "    x= Conv1D(16, 3, strides=1, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    #归一化作用\n",
    "    x= BatchNormalization()(x)\n",
    "    # 激活函数，自取正值部分\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(16, 3, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从651到325\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(24, 5, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(24, 5, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从325到162\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(32, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从162到81\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(64, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(64, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从81到40，有64个\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    #平铺将64*40送入全连接128\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # 随机损失10%的神经元数目\n",
    "    x = Dropout(0.1)(x)\n",
    "    x= Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x= Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "    #x = Dense(1, activation='relu')(x)\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "model = build_model()\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.00007),metrics=['accuracy'])\n",
    "plot_model(model,to_file=\"jn1.png\",show_shapes=True)\n",
    "model.summary()\n",
    "#SVG(model_to_dot(model).create(prog='dot',format='svg'))\n",
    "#plot_model(model,to_file=\"jl.png\",show_shapes=True)\n",
    "model.save(\"100yuan12cixun.h5\")\n",
    "# print(len(pres))\n",
    "#print(pres)\n",
    "# print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a1785e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'int'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-27d7c4fe0000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mdfy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Untitled\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Untitled 1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Untitled\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mzong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, to_append, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m   2692\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"to_append should be a Series or list/tuple of Series, got DataFrame\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2693\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2694\u001b[1;33m         return concat(\n\u001b[0m\u001b[0;32m   2695\u001b[0m             \u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2696\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \"\"\"\n\u001b[1;32m--> 285\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    368\u001b[0m                     \u001b[1;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                 )\n\u001b[1;32m--> 370\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'int'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,BatchNormalization,ReLU,Flatten,Dropout,GlobalAveragePooling1D,Dense,Activation\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn import linear_model # 导入线性模型包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors # 导入KNN包\n",
    "import sys\n",
    "import random\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\有缺陷\")\n",
    "you1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "pic_files = [fn for fn in you1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first1 = []\n",
    "you=[]\n",
    "for fn in pic_files:\n",
    "    dfy=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df2 = dfy[\"Untitled\"]\n",
    "    df2.append(1)\n",
    "    zong.append(abs(df2))\n",
    "print(zong)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f9aa5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.395714 0.415036 0.43951  ... 1.378874 1.373721 0.      ]\n",
      " [0.428561 0.444341 0.46334  ... 1.311891 1.308671 0.      ]\n",
      " [0.489425 0.512933 0.539662 ... 1.720871 1.717007 0.      ]\n",
      " ...\n",
      " [1.126402 1.157317 1.196282 ... 1.062318 1.053623 1.      ]\n",
      " [0.909031 0.939946 0.970539 ... 1.210452 1.205621 1.      ]\n",
      " [0.98503  1.012725 1.048792 ... 1.391433 1.411399 1.      ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33287\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "12/12 [==============================] - 4s 56ms/step - loss: 0.6635 - accuracy: 0.6406\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.5965 - accuracy: 0.7225 0s - loss: 0.5906 - accuracy: \n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.5743 - accuracy: 0.7068\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 1s 53ms/step - loss: 0.5475 - accuracy: 0.7487\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.5266 - accuracy: 0.7487\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.5361 - accuracy: 0.7696\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.5092 - accuracy: 0.7592\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 1s 52ms/step - loss: 0.4843 - accuracy: 0.7749\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.4500 - accuracy: 0.8010\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.4288 - accuracy: 0.8586\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.4097 - accuracy: 0.8482\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.3745 - accuracy: 0.8743\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 1s 54ms/step - loss: 0.3473 - accuracy: 0.8848\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.3041 - accuracy: 0.8854\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 1s 51ms/step - loss: 0.2739 - accuracy: 0.9110\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 1s 50ms/step - loss: 0.3025 - accuracy: 0.9058\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 1s 57ms/step - loss: 0.2452 - accuracy: 0.9267\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 18/500\n",
      " 5/12 [===========>..................] - ETA: 0s - loss: 0.2294 - accuracy: 0.9367"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-29ca45c08e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m#print(model.summary())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         model.fit_generator(\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLens\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1973\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1975\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1976\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *\n",
    "import xlrd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Batch_size = 16\n",
    "Long = 210\n",
    "Lens = int(210*0.99)\n",
    "\n",
    "def DataSet(MANIFEST_DIRS):\n",
    "#加载数据集\n",
    "    all_content = []\n",
    "    worksheet = xlrd.open_workbook(MANIFEST_DIRS)\n",
    "    sheet_names = worksheet.sheet_names()\n",
    "\n",
    "    sheet = worksheet.sheet_by_name(sheet_names[0])\n",
    "    cols = sheet.ncols\n",
    "\n",
    "    for ncol in range(cols):\n",
    "        all_content.append(sheet.col_values(ncol)[1:])\n",
    "    all_content = np.array(all_content)\n",
    "\n",
    "    return abs(all_content)\n",
    "\n",
    "all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\dianchi\\pilian\\zong.xls\"))\n",
    "all_contents=np.array(all_contents)\n",
    "print(all_contents)\n",
    "np.random.seed(2022)\n",
    "np.random.shuffle(all_contents)\n",
    "\n",
    "def convert2oneHot(index,Lens):#onehot编码\n",
    "    hot = np.zeros((Lens,))#np.zeros()返回来一个给定形状和类型的用0填充的数组；生成shape维度大小的全0数组。\n",
    "    hot[int(index)-1] = 1\n",
    "    return(hot)\n",
    "\n",
    "def xs_gen(batch_size=Batch_size,train=True):\n",
    "\n",
    "    if train:\n",
    "        img_list = all_contents[:Lens]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    else:\n",
    "        img_list = all_contents[Lens:]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "\n",
    "    # img_list = pd.read_csv(path)\n",
    "    # img_list = np.array(img_list)#np.array(x,dtype)：将x转化为一个类型为type的数组\n",
    "    # print(len(img_list))\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "            batch_list = img_list[i * batch_size: i * batch_size + batch_size]\n",
    "            batch_x = np.array([file for file in batch_list[:,0:-1]])\n",
    "            # print(\"label\",len(batch_list[1,1:-1]))\n",
    "            batch_y = np.array([convert2oneHot(label,2) for label in batch_list[:,-1]])\n",
    "            # print(batch_x, batch_y)\n",
    "            yield batch_x, batch_y\n",
    "# X_test, y_test= xs_gen(train=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataSet, ansSet, test_size = 0.1, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "#把标签转成oneHot\n",
    "\n",
    "\n",
    "\n",
    "TIME_PERIODS = 651\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    # 输入尺寸大小651,1，每次输入1个包含651特征的样本\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "    # 一维卷积，16个大小为3的卷积核，步长为1,\n",
    "    x= Conv1D(16, 3, strides=1, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    #归一化作用\n",
    "    x= BatchNormalization()(x)\n",
    "    # 激活函数，自取正值部分\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(16, 3, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从651到325\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(24, 5, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(24, 5, strides=1, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从325到162\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(32, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(32, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从162到81\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(64, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(64, 3, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    # 最大池化缩小一般，从81到40，有64个\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    #平铺将64*40送入全连接128\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # 随机损失10%的神经元数目\n",
    "    x = Dropout(0.1)(x)\n",
    "    x= Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x= Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "    #x = Dense(1, activation='relu')(x)\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "Train = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if Train == True:\n",
    "        train_iter = xs_gen()\n",
    "        val_iter = xs_gen(train=False)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=r'D:\\dianchi\\pilian\\500g6.h5',\n",
    "            monitor='val_loss', save_best_only=True,verbose=1)\n",
    "\n",
    "        model = build_model()\n",
    "        opt = Adam(0.0001)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "        #print(model.summary())\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=500,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )\n",
    "        model.save(r\"D:\\dianchi\\pilian\\500g6.h5\")\n",
    "        model.save_weights(r\"D:\\dianchi\\pilian\\finishMode500g6.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b91c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "178\n",
      "Epoch 1/100\n",
      "3/3 [==============================] - 5s 224ms/step - loss: 0.8055 - accuracy: 0.5281\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.6056 - accuracy: 0.7079\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.6090 - accuracy: 0.7022\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.5918 - accuracy: 0.6854\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.5671 - accuracy: 0.7528\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.5743 - accuracy: 0.6910\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 1s 209ms/step - loss: 0.5359 - accuracy: 0.7584\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 0.5093 - accuracy: 0.7753\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 1s 174ms/step - loss: 0.5517 - accuracy: 0.7697\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 1s 177ms/step - loss: 0.4847 - accuracy: 0.7921\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 0.4978 - accuracy: 0.7753\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.4670 - accuracy: 0.8034\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.4257 - accuracy: 0.8202\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.4609 - accuracy: 0.8258\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 1s 191ms/step - loss: 0.4188 - accuracy: 0.8258\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.4056 - accuracy: 0.8258\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.3816 - accuracy: 0.8596\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.3826 - accuracy: 0.8371\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.4038 - accuracy: 0.8146\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 1s 175ms/step - loss: 0.3904 - accuracy: 0.8315\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.3218 - accuracy: 0.8933\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.3145 - accuracy: 0.8876\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 1s 178ms/step - loss: 0.3194 - accuracy: 0.8820\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.3344 - accuracy: 0.8820\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.3538 - accuracy: 0.8483\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 0.2695 - accuracy: 0.9157\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.3066 - accuracy: 0.8989\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 1s 396ms/step - loss: 0.2158 - accuracy: 0.9101\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.2775 - accuracy: 0.8989\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 0.2785 - accuracy: 0.8989\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 1s 262ms/step - loss: 0.2772 - accuracy: 0.9101\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 1s 323ms/step - loss: 0.2223 - accuracy: 0.9326\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 1s 208ms/step - loss: 0.2488 - accuracy: 0.9213\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 1s 320ms/step - loss: 0.1865 - accuracy: 0.9326\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 1s 257ms/step - loss: 0.2752 - accuracy: 0.9101\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 0.2077 - accuracy: 0.9213\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 0.2445 - accuracy: 0.9213\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 1s 222ms/step - loss: 0.1775 - accuracy: 0.9494\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 1s 290ms/step - loss: 0.2848 - accuracy: 0.8989\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.2556 - accuracy: 0.9101\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 0.2013 - accuracy: 0.9382\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 1s 315ms/step - loss: 0.2566 - accuracy: 0.8876\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 0.2085 - accuracy: 0.8989\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.3297 - accuracy: 0.8876\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 0.2569 - accuracy: 0.8989\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 1s 196ms/step - loss: 0.1578 - accuracy: 0.9382\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.1799 - accuracy: 0.9494\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.2885 - accuracy: 0.8933\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 0.1312 - accuracy: 0.9607\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 1s 250ms/step - loss: 0.1395 - accuracy: 0.9494\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.1466 - accuracy: 0.9382\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 0.1183 - accuracy: 0.9607\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 1s 208ms/step - loss: 0.1056 - accuracy: 0.9719\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.1329 - accuracy: 0.9494\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 1s 195ms/step - loss: 0.1289 - accuracy: 0.9607\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.1214 - accuracy: 0.9438\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.1258 - accuracy: 0.9438\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 1s 179ms/step - loss: 0.0774 - accuracy: 0.9551\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0735 - accuracy: 0.9719\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 0.1837 - accuracy: 0.9326\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.1552 - accuracy: 0.9438\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 0.1019 - accuracy: 0.9551\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 0.1015 - accuracy: 0.9607\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.1091 - accuracy: 0.9607\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0920 - accuracy: 0.9663\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 0.0604 - accuracy: 0.9944\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 1s 175ms/step - loss: 0.1097 - accuracy: 0.94380s - loss: 0.0972 - accuracy: 0.\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0745 - accuracy: 0.9719\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0490 - accuracy: 0.9888\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 0.0479 - accuracy: 0.9888\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 1s 230ms/step - loss: 0.1045 - accuracy: 0.9663\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0391 - accuracy: 0.9831\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 1s 207ms/step - loss: 0.0766 - accuracy: 0.9719\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.1522 - accuracy: 0.9326\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0615 - accuracy: 0.9775\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.1950 - accuracy: 0.9382\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0656 - accuracy: 0.9719\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.1798 - accuracy: 0.9326\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 0.2735 - accuracy: 0.9270\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.1619 - accuracy: 0.9101\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.1246 - accuracy: 0.9551\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.1795 - accuracy: 0.9157\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0888 - accuracy: 0.9494\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 1s 177ms/step - loss: 0.2461 - accuracy: 0.9101\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 0.0586 - accuracy: 0.9775\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0677 - accuracy: 0.9775\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.1136 - accuracy: 0.9663\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 0.0668 - accuracy: 0.9775\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 0.0674 - accuracy: 0.9831\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 1s 320ms/step - loss: 0.0563 - accuracy: 0.9831\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 1s 333ms/step - loss: 0.0661 - accuracy: 0.9663\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 1s 468ms/step - loss: 0.0718 - accuracy: 0.9719\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 0.0600 - accuracy: 0.9888\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 1s 432ms/step - loss: 0.0314 - accuracy: 0.9888\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 0.1077 - accuracy: 0.9831\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 1s 400ms/step - loss: 0.0228 - accuracy: 0.9888\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0572 - accuracy: 0.9831\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 1s 424ms/step - loss: 0.0294 - accuracy: 0.9888\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 1s 430ms/step - loss: 0.0151 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,BatchNormalization,ReLU,Flatten,Dropout,GlobalAveragePooling1D,Dense,Activation\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn import linear_model # 导入线性模型包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors # 导入KNN包\n",
    "import sys\n",
    "import random\n",
    "import natsort\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *\n",
    "import xlrd\n",
    "from sklearn.model_selection import train_test_split\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\无缺陷\")\n",
    "wu1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "Batch_size = 64\n",
    "Long = 220\n",
    "Lens = int(220*0.2)\n",
    "zong =[]\n",
    "label = []\n",
    "pic_files = [fn for fn in wu1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first = []\n",
    "wu = []\n",
    "s = 0\n",
    "for fn in pic_files:\n",
    "    df=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df1 = df[\"Untitled\"]\n",
    "    s+=1\n",
    "    wu.append(df1)\n",
    "    if s>100:\n",
    "        zong.append(abs(df1))\n",
    "    else:\n",
    "        zong.append(abs(df1))\n",
    "    first.append(0)\n",
    "    label.append(0)\n",
    "os.chdir(\"../\")\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\有缺陷\")\n",
    "you1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "pic_files = [fn for fn in you1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first1 = []\n",
    "you=[]\n",
    "for fn in pic_files:\n",
    "    dfy=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df2 = dfy[\"Untitled\"]\n",
    "    you.append(df2)\n",
    "    zong.append(abs(df2))\n",
    "    first1.append(1)\n",
    "    label.append(1)\n",
    "os.chdir(\"../\")\n",
    "print(label)\n",
    "print(len(label))\n",
    "\n",
    "x = zong\n",
    "y = label\n",
    "\n",
    "seed=66\n",
    "random.seed(seed)\n",
    "random.shuffle(x)\n",
    "random.seed(seed)#一定得重复在写一遍,和上面的seed要相同,不然y_batch和x_batch打乱顺序会不一样\n",
    "random.shuffle(y)\n",
    "\n",
    "\n",
    "\n",
    "#df1=pd.read_csv('自制0mm缺陷总.csv',sep=\",\",header=[0])\n",
    "xy = tf.convert_to_tensor(x)\n",
    "yy = tf.convert_to_tensor(y)\n",
    "yy = tf.one_hot(yy,2)\n",
    "#xy = tf.one_hot(xy,2)\n",
    "\n",
    "\n",
    "\n",
    "# 特征数目\n",
    "TIME_PERIODS = 651\n",
    "# classnum=2为类别数，必须是2，因为你有两个类0,1\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "\n",
    "    x= Conv1D(16, 8, strides=2, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "opt = Adam(0.0001)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "        #print(model.summary())\n",
    "        \n",
    "        \n",
    "\n",
    "model.fit(xy,yy,epochs=100,batch_size=64)\n",
    "#pres = model.predict(x)\n",
    "SVG(model_to_dot(model).create(prog='dot',format='svg'))\n",
    "plot_model(model,to_file=\"651234jl.png\",show_shapes=True)       \n",
    "model.save(\"100cixunqu.h5\")\n",
    "# print(len(pres))\n",
    "#print(pres)\n",
    "# print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=600,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt], \n",
    "            )\n",
    "model.fit(xy,yy,\n",
    "            #generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=600,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f6b4d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,BatchNormalization,ReLU,Flatten,Dropout,GlobalAveragePooling1D,Dense,Activation\n",
    "#from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from sklearn import linear_model # 导入线性模型包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors # 导入KNN包\n",
    "import sys\n",
    "import random\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\无缺陷\")\n",
    "wu1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "zong = []\n",
    "label = []\n",
    "pic_files = [fn for fn in wu1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first = []\n",
    "wu = []\n",
    "s = 0\n",
    "for fn in pic_files:\n",
    "    df=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df1 = df[\"Untitled\"]\n",
    "    s+=1\n",
    "    wu.append(df1)\n",
    "    if s>100:\n",
    "        zong.append(abs(df1)-0.4)\n",
    "    else:\n",
    "        zong.append(abs(df1))\n",
    "    first.append(0)\n",
    "    label.append(0)\n",
    "os.chdir(\"../\")\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\有缺陷\")\n",
    "you1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "pic_files = [fn for fn in you1 if fn.endswith('.csv')]\n",
    "#print(pic_files)\n",
    "first1 = []\n",
    "you=[]\n",
    "for fn in pic_files:\n",
    "    dfy=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df2 = dfy[\"Untitled\"]\n",
    "    you.append(df2)\n",
    "    zong.append(abs(df2))\n",
    "    first1.append(1)\n",
    "    label.append(1)\n",
    "os.chdir(\"../\")\n",
    "print(label)\n",
    "print(len(label))\n",
    "\n",
    "x = zong\n",
    "y = pd.Series(label)\n",
    "\n",
    "seed=66\n",
    "random.seed(seed)\n",
    "random.shuffle(x)\n",
    "random.seed(seed)#一定得重复在写一遍,和上面的seed要相同,不然y_batch和x_batch打乱顺序会不一样\n",
    "random.shuffle(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78c8c7",
   "metadata": {},
   "source": [
    "# 打乱训练集\n",
    "#index = [i for i in range(len(y))]\n",
    "# 下面这种写法也可以\n",
    "# index = np.arange(len(dataset))\n",
    "#np.random.shuffle(index) # 打乱索引\n",
    "\n",
    "#x = x[index]\n",
    "#y = y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf6c781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.csv', '2.csv', '3.csv', '4.csv', '5.csv', '6.csv', '7.csv', '8.csv', '9.csv', '10.csv', '11.csv', '12.csv', '13.csv', '14.csv', '15.csv', '16.csv', '17.csv', '18.csv', '19.csv', '20.csv', '21.csv', '22.csv', '23.csv', '24.csv', '25.csv', '26.csv', '27.csv', '28.csv', '29.csv', '30.csv', '31.csv', '32.csv', '33.csv', '34.csv', '35.csv', '36.csv', '37.csv', '38.csv', '39.csv', '40.csv', '41.csv', '42.csv', '43.csv', '44.csv', '45.csv', '46.csv', '47.csv', '48.csv', '49.csv', '50.csv', '51.csv', '52.csv', '53.csv', '54.csv', '55.csv', '56.csv', '57.csv', '58.csv', '59.csv', '60.csv', '61.csv', '62.csv', '63.csv', '64.csv', '65.csv', '66.csv', '67.csv', '68.csv', '69.csv', '70.csv', '71.csv', '72.csv', '73.csv', '74.csv', '75.csv', '76.csv', '77.csv', '78.csv', '79.csv', '80.csv', '81.csv', '82.csv', '83.csv', '84.csv', '85.csv', '86.csv', '87.csv', '88.csv', '89.csv', '90.csv', '91.csv', '92.csv', '93.csv', '94.csv', '95.csv', '96.csv', '97.csv', '98.csv', '99.csv', '100.csv']\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "准确率： 0.7244897959183674\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QMessageBox,QInputDialog,QFileDialog\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import natsort\n",
    "\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\1121\") \n",
    "ppt_filename1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "zongyan = []\n",
    "labelyan = []\n",
    "pic_files = [fn for fn in ppt_filename1 if fn.endswith('.csv')]\n",
    "print(pic_files)\n",
    "first = []\n",
    "wu = []\n",
    "jishu = 0\n",
    "for fn in pic_files:\n",
    "    jishu +=1\n",
    "    df=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df1 = df[\"Untitled\"]\n",
    "    if jishu>53 and jishu<73:\n",
    "        zongyan.append(abs(df1)+0.2)\n",
    "    elif jishu>=73 and jishu<85:\n",
    "        zongyan.append(abs(df1)+0.1)\n",
    "    elif jishu>=85:\n",
    "        zongyan.append(abs(df1)+0.2)\n",
    "    elif jishu>0 and jishu <=25:\n",
    "        zongyan.append(abs(df1)+0.45)\n",
    "    elif jishu>26 and jishu<=52:\n",
    "        zongyan.append(abs(df1)+0.45)\n",
    "    \n",
    "zy = tf.convert_to_tensor(zongyan)\n",
    "#print(zongyan)\n",
    "\n",
    "\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\") \n",
    "model=load_model(\"2000yuancixun.h5\")\n",
    "pres = model.predict(zy)\n",
    "# print(pres)\n",
    "# print(len(pres))\n",
    "n=0\n",
    "abc = []\n",
    "for i in range(len(pres)):\n",
    "    maxvalue=max(pres[i])\n",
    "    abc.append(list(pres[i]).index(maxvalue))\n",
    "    print(list(pres[i]).index(maxvalue))\n",
    "    if list(pres[i]).index(maxvalue)==1:\n",
    "        n=n+1\n",
    "\n",
    "print(\"准确率：\",float(n/len(pres)))\n",
    "        \n",
    "\n",
    "jieg = pd.Series(abc)\n",
    "jieg.to_csv(\"15000g.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62686b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataSet(MANIFEST_DIRS):\n",
    "#加载数据集\n",
    "    all_content = []\n",
    "    worksheet = xlrd.open_workbook(MANIFEST_DIRS)\n",
    "    sheet_names = worksheet.sheet_names()\n",
    "\n",
    "    sheet = worksheet.sheet_by_name(sheet_names[0])\n",
    "    cols = sheet.ncols\n",
    "\n",
    "    for ncol in range(cols):\n",
    "        all_content.append(sheet.col_values(ncol)[1:])\n",
    "    all_content = np.array(all_content)\n",
    "\n",
    "    return abs(all_content)\n",
    "zongyan1=list(DataSet(MANIFEST_DIRS=r\"D:\\dianchi\\pilian\\zong1.xls\"))\n",
    "zongyan1 = tf.convert_to_tensor(np.array(zongyan1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f3ff12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "if jishu>=53 and jishu <79:\n",
    "        zongyan.append(abs(df1)-0.2)\n",
    "    elif jishu>=79 and jishu <=100:\n",
    "        zongyan.append(abs(df1)-0.2)\n",
    "    elif jishu>22 and jishu<52:\n",
    "        zongyan.append(abs(df1)-0.02)\n",
    "    else:\n",
    "result = []\n",
    "for i in pres:\n",
    "    if sum(i)<1:\n",
    "        result.append(0)\n",
    "    else:\n",
    "        result.append(1)\n",
    "print(result)\n",
    "res = pd.Series(result)\n",
    "\n",
    "res.to_csv(\"res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e430de8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.csv', '2.csv', '3.csv', '4.csv', '5.csv', '6.csv', '7.csv', '8.csv', '9.csv', '10.csv', '11.csv', '12.csv', '13.csv', '14.csv', '15.csv', '16.csv', '17.csv', '18.csv', '19.csv', '20.csv', '21.csv', '22.csv', '23.csv', '24.csv', '25.csv', '26.csv', '27.csv', '28.csv', '29.csv', '30.csv', '31.csv', '32.csv', '33.csv', '34.csv', '35.csv', '36.csv', '37.csv', '38.csv', '39.csv', '40.csv', '41.csv', '42.csv', '43.csv', '44.csv', '45.csv', '46.csv', '47.csv', '48.csv', '49.csv', '50.csv', '51.csv', '52.csv', '53.csv', '54.csv', '55.csv', '56.csv', '57.csv', '58.csv', '59.csv', '60.csv', '61.csv', '62.csv', '63.csv', '64.csv', '65.csv', '66.csv', '67.csv', '68.csv', '69.csv', '70.csv', '71.csv', '72.csv', '73.csv', '74.csv', '75.csv', '76.csv', '77.csv', '78.csv', '79.csv', '80.csv', '81.csv', '82.csv', '83.csv', '84.csv', '85.csv', '86.csv', '87.csv', '88.csv', '89.csv', '90.csv', '91.csv', '92.csv', '93.csv', '94.csv', '95.csv', '96.csv', '97.csv', '98.csv', '99.csv', '100.csv']\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "准确率： 0.67\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QMessageBox,QInputDialog,QFileDialog\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import natsort\n",
    "\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\\1121\") \n",
    "ppt_filename1 = natsort.natsorted(os.listdir(\"./\"),alg = natsort.ns.PATH)\n",
    "\n",
    "zongyan = []\n",
    "labelyan = []\n",
    "pic_files = [fn for fn in ppt_filename1 if fn.endswith('.csv')]\n",
    "print(pic_files)\n",
    "first = []\n",
    "wu = []\n",
    "jishu = 0\n",
    "for fn in pic_files:\n",
    "    jishu +=1\n",
    "    df=pd.read_csv(fn,sep=\"\\t\",skiprows=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],usecols=[\"Untitled\", \"Untitled 1\"])\n",
    "    df1 = df[\"Untitled\"]\n",
    "    if jishu>=53 and jishu <100:\n",
    "        zongyan.append(abs(df1))\n",
    "    else:\n",
    "        zongyan.append(abs(df1)+0.3)\n",
    "    \n",
    "zongyan = tf.convert_to_tensor(zongyan)\n",
    "#print(zongyan)\n",
    "os.chdir(r\"D:\\dianchi\\电池焊缝数据220组\") \n",
    "model=load_model(\"10000cixunqu.h5\")\n",
    "pres = model.predict(zongyan)\n",
    "# print(pres)\n",
    "# print(len(pres))\n",
    "n=0\n",
    "abc = []\n",
    "for i in range(len(pres)):\n",
    "    maxvalue=max(pres[i])\n",
    "    abc.append(list(pres[i]).index(maxvalue))\n",
    "    print(list(pres[i]).index(maxvalue))\n",
    "    if list(pres[i]).index(maxvalue)==1:\n",
    "        n=n+1\n",
    "\n",
    "print(\"准确率：\",float(n/len(pres)))\n",
    "        \n",
    "\n",
    "jieg = pd.Series(abc)\n",
    "jieg.to_csv(\"xin21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba176a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33287\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 5s 77ms/step - loss: 0.7226 - accuracy: 0.6302\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.6071 - accuracy: 0.7173\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 0.5555 - accuracy: 0.7173\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.4783 - accuracy: 0.8115\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.4876 - accuracy: 0.7958\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.4430 - accuracy: 0.8168\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.3755 - accuracy: 0.8429\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 0.3366 - accuracy: 0.8534\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.3262 - accuracy: 0.8639\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.2260 - accuracy: 0.9110\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 0.1714 - accuracy: 0.9424\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.1361 - accuracy: 0.9686\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1312 - accuracy: 0.9634\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 92ms/step - loss: 0.1185 - accuracy: 0.9635\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.1172 - accuracy: 0.9791\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.1220 - accuracy: 0.9529\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.1432 - accuracy: 0.9529\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.1770 - accuracy: 0.9529\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.1826 - accuracy: 0.9215\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 71ms/step - loss: 0.0957 - accuracy: 0.9738\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.0671 - accuracy: 0.9738\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.0656 - accuracy: 0.9738\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0470 - accuracy: 0.9895\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 0.0388 - accuracy: 0.9843\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0669 - accuracy: 0.9634\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.0829 - accuracy: 0.9895\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0873 - accuracy: 0.9688\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.1046 - accuracy: 0.9634\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 0.1115 - accuracy: 0.9581\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.0365 - accuracy: 0.9948\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.0231 - accuracy: 0.9948\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 77ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.0052 - accuracy: 1.0000 0s - loss: 0.0085 \n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 9.5427e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 126ms/step - loss: 6.3515e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 2s 126ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 2s 136ms/step - loss: 9.7869e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 5.6697e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 8.8398e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 2s 161ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 2s 197ms/step - loss: 8.8754e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 6.2194e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 6.4467e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 9.2010e-04 - accuracy: 1.0000 0s - loss: 2.4341e\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 5.9063e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 7.1655e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 4.1582e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 81ms/step - loss: 6.2978e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 3.6214e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 75ms/step - loss: 3.3457e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 4.2805e-04 - accuracy: 1.0000 0s - loss: 4.2805e-04 - accuracy: 1.00\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 4.7360e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 5.0915e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 5.8086e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.9821e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 4.8715e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 4.7877e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 87ms/step - loss: 4.0854e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 5.5670e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 4.5425e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 4.2945e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 67ms/step - loss: 4.4123e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 3.1584e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 3.7417e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 1s 70ms/step - loss: 3.8581e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 2.9726e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 1s 74ms/step - loss: 3.4557e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 1s 73ms/step - loss: 2.8931e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 1s 78ms/step - loss: 3.8265e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 3.1927e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 5.6041e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 3.7560e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 1s 68ms/step - loss: 2.9461e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 2.0017e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 73ms/step - loss: 1.9999e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.1462e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 3.5619e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 3.5183e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 1.4085e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 1s 79ms/step - loss: 1.8991e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 1s 72ms/step - loss: 4.6678e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 1s 76ms/step - loss: 1.9585e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 4.5018e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 1s 69ms/step - loss: 2.3161e-04 - accuracy: 1.0000\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 1.8937e-04 - accuracy: 1.0000 0s - loss: 1.9971e-04 - accura\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *\n",
    "import xlrd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Batch_size = 16\n",
    "Long = 210\n",
    "Lens = int(210*0.99)\n",
    "\n",
    "def DataSet(MANIFEST_DIRS):\n",
    "#加载数据集\n",
    "    all_content = []\n",
    "    worksheet = xlrd.open_workbook(MANIFEST_DIRS)\n",
    "    sheet_names = worksheet.sheet_names()\n",
    "\n",
    "    sheet = worksheet.sheet_by_name(sheet_names[0])\n",
    "    cols = sheet.ncols\n",
    "\n",
    "    for ncol in range(cols):\n",
    "        all_content.append(sheet.col_values(ncol)[1:])\n",
    "    all_content = np.array(all_content)\n",
    "\n",
    "    return all_content\n",
    "\n",
    "all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\dianchi\\pilian\\zong.xls\"))\n",
    "all_contents=np.array(all_contents)\n",
    "print(len(all_contents))\n",
    "np.random.seed(2022)\n",
    "np.random.shuffle(all_contents)\n",
    "\n",
    "def convert2oneHot(index,Lens):#onehot编码\n",
    "    hot = np.zeros((Lens,))#np.zeros()返回来一个给定形状和类型的用0填充的数组；生成shape维度大小的全0数组。\n",
    "    hot[int(index)-1] = 1\n",
    "    return(hot)\n",
    "\n",
    "def xs_gen(batch_size=Batch_size,train=True):\n",
    "\n",
    "    if train:\n",
    "        img_list = all_contents[:Lens]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    else:\n",
    "        img_list = all_contents[Lens:]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "\n",
    "    # img_list = pd.read_csv(path)\n",
    "    # img_list = np.array(img_list)#np.array(x,dtype)：将x转化为一个类型为type的数组\n",
    "    # print(len(img_list))\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "            batch_list = img_list[i * batch_size: i * batch_size + batch_size]\n",
    "            batch_x = np.array([file for file in batch_list[:,1:-1]])\n",
    "            # print(\"label\",len(batch_list[1,1:-1]))\n",
    "            batch_y = np.array([convert2oneHot(label,2) for label in batch_list[:,-1]])\n",
    "            # print(batch_x, batch_y)\n",
    "            yield batch_x, batch_y\n",
    "# X_test, y_test= xs_gen(train=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataSet, ansSet, test_size = 0.1, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "#把标签转成oneHot\n",
    "\n",
    "\n",
    "\n",
    "TIME_PERIODS = 651\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "\n",
    "    x= Conv1D(16, 3, strides=2, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "Train = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if Train == True:\n",
    "        train_iter = xs_gen()\n",
    "        val_iter = xs_gen(train=False)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=r'D:\\dianchi\\pilian\\gp6.h5',\n",
    "            monitor='val_loss', save_best_only=True,verbose=1)\n",
    "\n",
    "        model = build_model()\n",
    "        opt = Adam(0.0001)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "        #print(model.summary())\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=100,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )\n",
    "        SVG(model_to_dot(model).create(prog='dot',format='svg'))\n",
    "        plot_model(model,to_file=\"7651234jl.png\",show_shapes=True)       \n",
    "        model.save(r\"D:\\dianchi\\pilian\\gp6.h5\")\n",
    "        model.save_weights(r\"D:\\dianchi\\pilian\\finishModelgp6.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b300102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 3s 191ms/step - loss: 0.6899 - accuracy: 0.7500 - val_loss: 0.6849 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68494, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 106ms/step - loss: 0.8151 - accuracy: 0.5694 - val_loss: 0.6872 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.68494\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.5693 - accuracy: 0.7500 - val_loss: 0.6765 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68494 to 0.67655, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.4671 - accuracy: 0.7917 - val_loss: 0.6680 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67655 to 0.66804, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.5457 - accuracy: 0.7361 - val_loss: 0.6553 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.66804 to 0.65525, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 108ms/step - loss: 0.3700 - accuracy: 0.8750 - val_loss: 0.6458 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.65525 to 0.64580, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.4042 - accuracy: 0.8125 - val_loss: 0.6377 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.64580 to 0.63772, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.3286 - accuracy: 0.8472 - val_loss: 0.6328 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.63772 to 0.63276, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.3553 - accuracy: 0.8472 - val_loss: 0.6224 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.63276 to 0.62238, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3080 - accuracy: 0.8472 - val_loss: 0.6157 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.62238 to 0.61571, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2903 - accuracy: 0.8472 - val_loss: 0.6168 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.61571\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3119 - accuracy: 0.8611 - val_loss: 0.6232 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.61571\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.3654 - accuracy: 0.9125 - val_loss: 0.6516 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.61571\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.4058 - accuracy: 0.8611 - val_loss: 0.6337 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.61571\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2437 - accuracy: 0.8889 - val_loss: 0.6107 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.61571 to 0.61072, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.3360 - accuracy: 0.8611 - val_loss: 0.6415 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.61072\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.3156 - accuracy: 0.8472 - val_loss: 0.6714 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.61072\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2562 - accuracy: 0.9167 - val_loss: 0.6419 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.61072\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.1968 - accuracy: 0.9250 - val_loss: 0.6521 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61072\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.1422 - accuracy: 0.9306 - val_loss: 0.7209 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.61072\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.0938 - accuracy: 0.9722 - val_loss: 0.7787 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.61072\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.1064 - accuracy: 0.9444 - val_loss: 0.9291 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.61072\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.1845 - accuracy: 0.9167 - val_loss: 0.9358 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.61072\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 107ms/step - loss: 0.1032 - accuracy: 0.9444 - val_loss: 1.0815 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.61072\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.0689 - accuracy: 0.9625 - val_loss: 1.1522 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.61072\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.0884 - accuracy: 0.9583 - val_loss: 1.1772 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.61072\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.1105 - accuracy: 0.9583 - val_loss: 1.3929 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.61072\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2513 - accuracy: 0.8889 - val_loss: 1.3522 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.61072\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.3530 - accuracy: 0.9028 - val_loss: 1.1776 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.61072\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.4790 - accuracy: 0.8194 - val_loss: 1.0523 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.61072\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.5149 - accuracy: 0.8250 - val_loss: 0.7011 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.61072\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.2786 - accuracy: 0.9028 - val_loss: 0.5924 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.61072 to 0.59241, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.2766 - accuracy: 0.9306 - val_loss: 0.6350 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.59241\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.1446 - accuracy: 0.9722 - val_loss: 0.7175 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.59241\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.1572 - accuracy: 0.9306 - val_loss: 0.7433 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.59241\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0677 - accuracy: 0.9861 - val_loss: 0.8532 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.59241\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 1.0517 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.59241\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0190 - accuracy: 0.9861 - val_loss: 1.1406 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.59241\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0643 - accuracy: 0.9722 - val_loss: 1.2916 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.59241\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.1899 - accuracy: 0.9306 - val_loss: 1.4301 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.59241\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 95ms/step - loss: 0.1212 - accuracy: 0.9583 - val_loss: 1.4731 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.59241\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.1061 - accuracy: 0.9444 - val_loss: 1.4865 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.59241\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.1265 - accuracy: 0.9250 - val_loss: 1.4386 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.59241\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0685 - accuracy: 0.9861 - val_loss: 1.4411 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.59241\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.1449 - accuracy: 0.9444 - val_loss: 1.3430 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.59241\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.0811 - accuracy: 0.9583 - val_loss: 1.2369 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.59241\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.1053 - accuracy: 0.9722 - val_loss: 1.7970 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.59241\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0650 - accuracy: 0.9861 - val_loss: 1.6427 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.59241\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.1077 - accuracy: 0.9750 - val_loss: 1.0236 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.59241\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 107ms/step - loss: 0.1139 - accuracy: 0.9861 - val_loss: 0.8633 - val_accuracy: 0.5410\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.59241\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.0415 - accuracy: 0.9861 - val_loss: 0.8539 - val_accuracy: 0.5492\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.59241\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.1177 - accuracy: 0.9861 - val_loss: 0.9979 - val_accuracy: 0.6393\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.59241\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0412 - accuracy: 0.9861 - val_loss: 0.9906 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.59241\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0601 - accuracy: 0.9722 - val_loss: 1.0868 - val_accuracy: 0.5984\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.59241\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.0144 - accuracy: 0.9875 - val_loss: 1.1802 - val_accuracy: 0.5492\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.59241\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.2745 - val_accuracy: 0.5328\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.59241\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.3779 - val_accuracy: 0.5328\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.59241\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.4998 - val_accuracy: 0.5328\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.59241\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.6231 - val_accuracy: 0.5246\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.59241\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.7345 - val_accuracy: 0.5246\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.59241\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.8449 - val_accuracy: 0.5246\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.59241\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.9442 - val_accuracy: 0.5410\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.59241\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.0406 - val_accuracy: 0.5410\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.59241\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.1430 - val_accuracy: 0.5492\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.59241\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 7.8700e-04 - accuracy: 1.0000 - val_loss: 2.2462 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.59241\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 3.6927e-04 - accuracy: 1.0000 - val_loss: 2.3277 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.59241\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.3847 - val_accuracy: 0.5656\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.59241\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 4.2585e-04 - accuracy: 1.0000 - val_loss: 2.4359 - val_accuracy: 0.5656\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.59241\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 2.0583e-04 - accuracy: 1.0000 - val_loss: 2.4745 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.59241\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 5.0098e-04 - accuracy: 1.0000 - val_loss: 2.5098 - val_accuracy: 0.5738\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.59241\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 1.5407e-04 - accuracy: 1.0000 - val_loss: 2.5459 - val_accuracy: 0.5738\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.59241\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 1.8800e-04 - accuracy: 1.0000 - val_loss: 2.5760 - val_accuracy: 0.5738\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.59241\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 1.6188e-04 - accuracy: 1.0000 - val_loss: 2.5945 - val_accuracy: 0.5984\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.59241\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 2.6047e-04 - accuracy: 1.0000 - val_loss: 2.6169 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.59241\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 3.8127e-04 - accuracy: 1.0000 - val_loss: 2.6354 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.59241\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 3.1841e-04 - accuracy: 1.0000 - val_loss: 2.6661 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.59241\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 7.9851e-05 - accuracy: 1.0000 - val_loss: 2.6996 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.59241\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 1.7518e-04 - accuracy: 1.0000 - val_loss: 2.7322 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.59241\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 1.6586e-04 - accuracy: 1.0000 - val_loss: 2.7591 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.59241\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 3.5260e-04 - accuracy: 1.0000 - val_loss: 2.7878 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.59241\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 9.2345e-05 - accuracy: 1.0000 - val_loss: 2.8099 - val_accuracy: 0.6230\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.59241\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 1.5571e-04 - accuracy: 1.0000 - val_loss: 2.8299 - val_accuracy: 0.6311\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.59241\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 1.2227e-04 - accuracy: 1.0000 - val_loss: 2.8542 - val_accuracy: 0.6311\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.59241\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 7.5845e-05 - accuracy: 1.0000 - val_loss: 2.8731 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.59241\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 2.8922e-04 - accuracy: 1.0000 - val_loss: 2.8783 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.59241\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 4.3392e-05 - accuracy: 1.0000 - val_loss: 2.8996 - val_accuracy: 0.6230\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.59241\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 3.2081e-05 - accuracy: 1.0000 - val_loss: 2.9162 - val_accuracy: 0.6230\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.59241\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 8.4546e-05 - accuracy: 1.0000 - val_loss: 2.9392 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.59241\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 7.9763e-05 - accuracy: 1.0000 - val_loss: 2.9765 - val_accuracy: 0.6230\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.59241\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.7696e-04 - accuracy: 1.0000 - val_loss: 3.0075 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.59241\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 3.3988e-04 - accuracy: 1.0000 - val_loss: 3.0286 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.59241\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 5.6427e-05 - accuracy: 1.0000 - val_loss: 3.0544 - val_accuracy: 0.6230\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.59241\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 1.7239e-04 - accuracy: 1.0000 - val_loss: 3.0708 - val_accuracy: 0.5984\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.59241\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 2.6006e-04 - accuracy: 1.0000 - val_loss: 3.0922 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.59241\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 6.0189e-05 - accuracy: 1.0000 - val_loss: 3.1287 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.59241\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 4.4508e-05 - accuracy: 1.0000 - val_loss: 3.1543 - val_accuracy: 0.6066\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.59241\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 1.3535e-04 - accuracy: 1.0000 - val_loss: 3.1627 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.59241\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 108ms/step - loss: 1.4558e-04 - accuracy: 1.0000 - val_loss: 3.1812 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.59241\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 6.7193e-05 - accuracy: 1.0000 - val_loss: 3.1942 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.59241\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 1.0207e-04 - accuracy: 1.0000 - val_loss: 3.2031 - val_accuracy: 0.6148\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.59241\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No such layer: dense_2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-f07892056e71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# 达到降维的目的，从原有的32767降到128维\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"D:\\dianchi\\pilian\\zongfinishModel.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m     \u001b[0mvisualization_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dense_2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m     \u001b[0mpres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualization_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLens\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# ohpres = np.argmax(pres, axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mget_layer\u001b[1;34m(self, name, index)\u001b[0m\n\u001b[0;32m   2566\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2567\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2568\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such layer: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2569\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Provide either a layer name or layer index.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No such layer: dense_2."
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *\n",
    "import xlrd\n",
    "from tensorflow.keras.models import *\n",
    "#超参数设置，Batch_size为训练时每次的数据量，总共迭代600次，训练数据每次共300数据量，由于网络输入规定为32个，所以1次迭代要进行10次。\n",
    "\"\"\"\n",
    "Batch_size = 16\n",
    "Long = 300\n",
    "Lens = int(300*0.2)\n",
    "\"\"\"\n",
    "Batch_size = 16\n",
    "Long = 220\n",
    "\n",
    "Lens = int(220*0.4)\n",
    "TestLens = int(220 * 0.6)\n",
    "def DataSet(MANIFEST_DIRS):\n",
    "#加载数据集，将 列 转化为 行\n",
    "    all_content = []\n",
    "    worksheet = xlrd.open_workbook(MANIFEST_DIRS)\n",
    "    sheet_names = worksheet.sheet_names()#获取所有sheet的名称，以列表方式显示\n",
    "\n",
    "    sheet = worksheet.sheet_by_name(sheet_names[0])#通过sheet名称获取所需sheet对象\n",
    "    cols = sheet.ncols#获取某sheet中的有效列数\n",
    "\n",
    "    for ncol in range(cols):\n",
    "        #all_content.append(sheet.col_values(ncol)[1:])#1改为0\n",
    "        all_content.append(sheet.col_values(ncol)[0:])\n",
    "    all_content = np.array(all_content)\n",
    "\n",
    "    return all_content\n",
    "\n",
    "all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\dianchi\\pilian\\zong.xls\"))\n",
    "# a = list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_0.xls\"))\n",
    "#all_contents = []\n",
    "#for i in range(10):\n",
    "#    all_contents += list(DataSet(MANIFEST_DIRS=r\"D:\\DeepLearning\\Bearing-fault-detection\\工况3数据集\\Vertical_3_\"+ str(i) +\".xls\"))\n",
    "# all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_0.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_1.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_2.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_3.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_4.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_5.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_6.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_7.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_8.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_9.xls\"))\n",
    "#print(all_contents)\n",
    "all_contents=np.array(all_contents)\n",
    "print(len(all_contents))\n",
    "\n",
    "np.random.seed(2021)#np. random. seed() 函数用于生成指定随机数。\n",
    "np.random.shuffle(all_contents)\n",
    "# numpy.random. shuffle(x)。shuffle()是不能直接访问的，可以导入numpy.random模块，然后通过 numpy.random 静态对象调用该方法，shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值(是对列表x中的所有元素随机打乱顺序，若x不是列表，则报错)。\n",
    "\n",
    "#主要作用用于与神经网络经softmax输出的预测值最大值对应的索引，相比较\n",
    "def convert2oneHot(index,Lens):#onehot编码，假如有6个类别，0类别 对应的onehot编码为 000000,1 000010\n",
    "    hot = np.zeros((Lens,))#np.zeros()返回来一个给定形状和类型的用0填充的数组；生成shape维度大小的全0数组。\n",
    "    hot[int(index)-1] = 1\n",
    "    #hot[int(index) ] = 1\n",
    "    return(hot)\n",
    "\n",
    "def xs_gen(batch_size=Batch_size,train=True):\n",
    "    if train:\n",
    "        img_list = all_contents[:Lens]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    else:\n",
    "        img_list = all_contents[Lens:]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    # img_list = pd.read_csv(path)\n",
    "    # img_list = np.array(img_list)#np.array(x,dtype)：将x转化为一个类型为type的数组\n",
    "    # print(len(img_list))\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "            batch_list = img_list[i * batch_size: i * batch_size + batch_size]\n",
    "            batch_x = np.array([file for file in batch_list[:,1:-1]])\n",
    "            # print(\"label\",len(batch_list[1,1:-1]))\n",
    "            #batch_y = np.array([convert2oneHot(label,6) for label in batch_list[:,-1]])#6改为10\n",
    "            batch_y = np.array([convert2oneHot(label, 2) for label in batch_list[:, -1]])\n",
    "            # print(batch_x, batch_y)\n",
    "            yield batch_x, batch_y#yield关键字使生成器函数执行暂停，yield关键字后面的表达式的值返回给生成器的调用者。\n",
    "# X_test, y_test= xs_gen(train=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataSet, ansSet, test_size = 0.1, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "#把标签转成oneHot\n",
    "\n",
    "\n",
    "\n",
    "TIME_PERIODS = 651#时间段\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "    #tf.keras.layers.Input()输入层解析\n",
    "    #shape：输入的形状，tuple类型。不含batch_size；tuple的元素可以为None类型数据，表示未知的或者说任意的，一般这里不用None\n",
    "    #dtype：数据类型，在大多数时候，我们需要的数据类型为tf.float32，因为在精度满足的情况下，float32运算更快。\n",
    "    x= Conv1D(16, 8, strides=2, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    ##16-输出的维度；卷积核的空域或时域窗长度,卷积核的尺寸，卷积核的大小为8；stride-卷积步长；\n",
    "    x= BatchNormalization()(x)\n",
    "    #BatchNormalization批量归一化（BN）是神经网络的标准化方法/层通常BN神经网络输入被归一化[0,1]或[-1,1]范围，或者意味着均值为0和方差等于1。\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)#MaxPooling1D：是在steps维度（也就是第二维）求最大值。但是限制每一步的池化的大小。\n",
    "    # 比如，输入数据维度是[10, 4, 10]，池化层大小pooling_size=2，步长stride=1，那么经过MaxPooling(pooling_size=2, stride=1)后，输出数据维度是[10, 3, 10]。\n",
    "\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    x = GlobalAveragePooling1D()(x)#GlobalMaxPooling1D:在steps维度（也就是第二维）对整个数据求最大值。比如说输入数据维度是[10, 4, 10]，那么进过全局池化后，输出数据的维度则变成[10, 10]。\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)#防过拟合\n",
    "    x= Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "Train = True\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def random_gen(train=True):\n",
    "\n",
    "    if train:\n",
    "        img_list1 = all_contents[:Lens]\n",
    "    else:\n",
    "        img_list1 = all_contents[Lens:]\n",
    "    while True:\n",
    "        batch_x1 = np.array([file for file in img_list1[:, 1:-1]])\n",
    "        #batch_y1 = np.array([label-1 for label in img_list1[:, -1]])\n",
    "        batch_y1 = np.array([label  for label in img_list1[:, -1]])\n",
    "        return batch_x1, batch_y1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if Train == True:\n",
    "        train_iter = xs_gen()\n",
    "        val_iter = xs_gen(train=False)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=r'D:\\dianchi\\pilian\\jn.h5',\n",
    "            monitor='val_loss', save_best_only=True,verbose=20)\n",
    "\n",
    "        model = build_model()\n",
    "\n",
    "        opt = Adam(0.0005)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#         print(model.summary())\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=100,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )\n",
    "        model.save(r\"D:\\dianchi\\pilian\\zongfinishModel.h5\")\n",
    "        # model.save_weights(r\"E:\\pycharm2021\\pycharm\\PyCharm2021.1.1\\project\\Bearing-fault-detection\\weight/finishModel11.h5\")\n",
    "    train_iter = xs_gen()\n",
    "    test_iter = xs_gen(train=False)\n",
    "    # print('test_iter的值为',test_iter)\n",
    "    train, label = random_gen()\n",
    "    # print('label的值为', label)\n",
    "    testtrain, testlabel = random_gen(train=False)\n",
    "    # print('testlabel的值为', testlabel)\n",
    "\n",
    "    # 达到降维的目的，从原有的32767降到128维\n",
    "    model = tf.keras.models.load_model(filepath=r\"D:\\dianchi\\pilian\\zongfinishModel.h5\")\n",
    "    visualization_model = Model(inputs=model.input, outputs=model.get_layer('dense_2').output)\n",
    "    pres = visualization_model.predict_generator(generator=train_iter, steps=math.ceil(Lens / Batch_size), verbose=1)\n",
    "    # ohpres = np.argmax(pres, axis=1)\n",
    "    # print(pres)\n",
    "    models = RandomForestClassifier(n_estimators=25,\n",
    "                                    n_jobs=2, min_samples_split=2, random_state=1, )\n",
    "    #\n",
    "    # # Fit on training data\n",
    "    models.fit(pres, label)\n",
    "    # print(pres.shape)\n",
    "\n",
    "    # Pre = models.predict(pres)\n",
    "    # num = 0\n",
    "    # for n in range(len(label)):\n",
    "    #     if int(Pre[n])== int(label[n] ):\n",
    "    #         num = num + 1\n",
    "    # print(\"准确率：\", num / len(label))\n",
    "\n",
    "    test_pres = visualization_model.predict_generator(generator=test_iter, steps=math.ceil(TestLens / Batch_size),\n",
    "                                                      verbose=1)\n",
    "    Pre = models.predict(test_pres)\n",
    "    print(Pre)\n",
    "    print(testlabel)\n",
    "    #\n",
    "    num = 0\n",
    "    # for n in range(len(testlabel)):\n",
    "    for n in range(len(testlabel) - 1):\n",
    "        if int(Pre[n]) == int(testlabel[n]):\n",
    "            num = num + 1\n",
    "    print(\"准确率：\", num / len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1494f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 207ms/step - loss: 0.8570 - accuracy: 0.6500 - val_loss: 0.6757 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67570, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.5654 - accuracy: 0.7778 - val_loss: 0.6878 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.67570\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6550 - accuracy: 0.7083 - val_loss: 0.6755 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67570 to 0.67552, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.4754 - accuracy: 0.7778 - val_loss: 0.6766 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67552\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.4647 - accuracy: 0.8472 - val_loss: 0.6756 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.67552\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.5806 - accuracy: 0.7500 - val_loss: 0.6650 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67552 to 0.66505, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.4075 - accuracy: 0.8875 - val_loss: 0.6723 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66505\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.4635 - accuracy: 0.7500 - val_loss: 0.6549 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66505 to 0.65486, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.2964 - accuracy: 0.8611 - val_loss: 0.6293 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.65486 to 0.62928, saving model to D:\\dianchi\\pilian\\jn.h5\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.4212 - accuracy: 0.8611 - val_loss: 0.6417 - val_accuracy: 0.6967\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.62928\n",
      "6/6 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33287\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-65ac33124be0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;31m# # Fit on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[1;31m# print(pres.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             )\n\u001b[1;32m--> 304\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    305\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    657\u001b[0m                     \"into decimal numbers with dtype='numeric'\") from e\n\u001b[0;32m    658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[0;32m    660\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import *\n",
    "import xlrd\n",
    "from tensorflow.keras.models import *\n",
    "#超参数设置，Batch_size为训练时每次的数据量，总共迭代600次，训练数据每次共300数据量，由于网络输入规定为32个，所以1次迭代要进行10次。\n",
    "\"\"\"\n",
    "Batch_size = 16\n",
    "Long = 300\n",
    "Lens = int(300*0.2)\n",
    "\"\"\"\n",
    "Batch_size = 16\n",
    "Long = 220\n",
    "\n",
    "Lens = int(220*0.4)\n",
    "TestLens = int(220 * 0.6)\n",
    "def DataSet(MANIFEST_DIRS):\n",
    "#加载数据集，将 列 转化为 行\n",
    "    all_content = []\n",
    "    worksheet = xlrd.open_workbook(MANIFEST_DIRS)\n",
    "    sheet_names = worksheet.sheet_names()#获取所有sheet的名称，以列表方式显示\n",
    "\n",
    "    sheet = worksheet.sheet_by_name(sheet_names[0])#通过sheet名称获取所需sheet对象\n",
    "    cols = sheet.ncols#获取某sheet中的有效列数\n",
    "\n",
    "    for ncol in range(cols):\n",
    "        #all_content.append(sheet.col_values(ncol)[1:])#1改为0\n",
    "        all_content.append(sheet.col_values(ncol)[0:])\n",
    "    all_content = np.array(all_content)\n",
    "\n",
    "    return all_content\n",
    "\n",
    "all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\dianchi\\pilian\\zong.xls\"))\n",
    "# a = list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_0.xls\"))\n",
    "#all_contents = []\n",
    "#for i in range(10):\n",
    "#    all_contents += list(DataSet(MANIFEST_DIRS=r\"D:\\DeepLearning\\Bearing-fault-detection\\工况3数据集\\Vertical_3_\"+ str(i) +\".xls\"))\n",
    "# all_contents=list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_0.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_1.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_2.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_3.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_4.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_5.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_6.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_7.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_8.xls\"))+list(DataSet(MANIFEST_DIRS=r\"D:\\数据集\\西交数据集最终版\\2-数据集\\2-分类方式（列）\\Horizontal_1_9.xls\"))\n",
    "#print(all_contents)\n",
    "all_contents=np.array(all_contents)\n",
    "print(len(all_contents))\n",
    "\n",
    "np.random.seed(2021)#np. random. seed() 函数用于生成指定随机数。\n",
    "np.random.shuffle(all_contents)\n",
    "# numpy.random. shuffle(x)。shuffle()是不能直接访问的，可以导入numpy.random模块，然后通过 numpy.random 静态对象调用该方法，shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值(是对列表x中的所有元素随机打乱顺序，若x不是列表，则报错)。\n",
    "\n",
    "#主要作用用于与神经网络经softmax输出的预测值最大值对应的索引，相比较\n",
    "def convert2oneHot(index,Lens):#onehot编码，假如有6个类别，0类别 对应的onehot编码为 000000,1 000010\n",
    "    hot = np.zeros((Lens,))#np.zeros()返回来一个给定形状和类型的用0填充的数组；生成shape维度大小的全0数组。\n",
    "    hot[int(index)-1] = 1\n",
    "    #hot[int(index) ] = 1\n",
    "    return(hot)\n",
    "\n",
    "def xs_gen(batch_size=Batch_size,train=True):\n",
    "    if train:\n",
    "        img_list = all_contents[:Lens]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    else:\n",
    "        img_list = all_contents[Lens:]\n",
    "        steps = math.ceil(len(img_list) / batch_size)    # 确定每轮有多少个batch\n",
    "    # img_list = pd.read_csv(path)\n",
    "    # img_list = np.array(img_list)#np.array(x,dtype)：将x转化为一个类型为type的数组\n",
    "    # print(len(img_list))\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "            batch_list = img_list[i * batch_size: i * batch_size + batch_size]\n",
    "            batch_x = np.array([file for file in batch_list[:,1:-1]])\n",
    "            # print(\"label\",len(batch_list[1,1:-1]))\n",
    "            #batch_y = np.array([convert2oneHot(label,2) for label in batch_list[:,-1]])#6改为10\n",
    "            batch_y = np.array([convert2oneHot(label, 2) for label in batch_list[:, -1]])\n",
    "            # print(batch_x, batch_y)\n",
    "            yield batch_x, batch_y#yield关键字使生成器函数执行暂停，yield关键字后面的表达式的值返回给生成器的调用者。\n",
    "# X_test, y_test= xs_gen(train=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataSet, ansSet, test_size = 0.1, random_state=2)\n",
    "\n",
    "\n",
    "\n",
    "#把标签转成oneHot\n",
    "\n",
    "\n",
    "\n",
    "TIME_PERIODS = 651#时间段\n",
    "def build_model (classnum=2):\n",
    "\n",
    "    xin=tf.keras.layers.Input(shape=(TIME_PERIODS,1),dtype=tf.float32)\n",
    "    #tf.keras.layers.Input()输入层解析\n",
    "    #shape：输入的形状，tuple类型。不含batch_size；tuple的元素可以为None类型数据，表示未知的或者说任意的，一般这里不用None\n",
    "    #dtype：数据类型，在大多数时候，我们需要的数据类型为tf.float32，因为在精度满足的情况下，float32运算更快。\n",
    "    x= Conv1D(16, 8, strides=2, input_shape=(TIME_PERIODS, 1))(xin)\n",
    "    ##16-输出的维度；卷积核的空域或时域窗长度,卷积核的尺寸，卷积核的大小为8；stride-卷积步长；\n",
    "    x= BatchNormalization()(x)\n",
    "    #BatchNormalization批量归一化（BN）是神经网络的标准化方法/层通常BN神经网络输入被归一化[0,1]或[-1,1]范围，或者意味着均值为0和方差等于1。\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)#MaxPooling1D：是在steps维度（也就是第二维）求最大值。但是限制每一步的池化的大小。\n",
    "    # 比如，输入数据维度是[10, 4, 10]，池化层大小pooling_size=2，步长stride=1，那么经过MaxPooling(pooling_size=2, stride=1)后，输出数据维度是[10, 3, 10]。\n",
    "\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(64, 4, strides=2, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(256, 4, strides=2, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = Conv1D(512, 2, strides=1, padding=\"same\")(x)\n",
    "    x= BatchNormalization()(x)\n",
    "    x= Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    \"\"\"model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\"\"\"\n",
    "    x = GlobalAveragePooling1D()(x)#GlobalMaxPooling1D:在steps维度（也就是第二维）对整个数据求最大值。比如说输入数据维度是[10, 4, 10]，那么进过全局池化后，输出数据的维度则变成[10, 10]。\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)#防过拟合\n",
    "    x= Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x= Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(classnum, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(xin, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "Train = True\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def random_gen(train=True):\n",
    "\n",
    "    if train:\n",
    "        img_list1 = all_contents[:Lens]\n",
    "    else:\n",
    "        img_list1 = all_contents[Lens:]\n",
    "    while True:\n",
    "        batch_x1 = np.array([file for file in img_list1[:, 1:-1]])\n",
    "        #batch_y1 = np.array([label-1 for label in img_list1[:, -1]])\n",
    "        batch_y1 = np.array([label  for label in img_list1[:, -1]])\n",
    "        return batch_x1, batch_y1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if Train == True:\n",
    "        train_iter = xs_gen()\n",
    "        val_iter = xs_gen(train=False)\n",
    "\n",
    "        ckpt = ModelCheckpoint(\n",
    "            filepath=r'D:\\dianchi\\pilian\\jn.h5',\n",
    "            monitor='val_loss', save_best_only=True,verbose=20)\n",
    "\n",
    "        model = build_model()\n",
    "\n",
    "        opt = Adam(0.0005)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "        #print(model.summary())\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=10,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            validation_steps = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )\n",
    "        model.save(r\"D:\\dianchi\\pilian\\zongfinishModel.h5\")\n",
    "        # model.save_weights(r\"E:\\pycharm2021\\pycharm\\PyCharm2021.1.1\\project\\Bearing-fault-detection\\weight/finishModel11.h5\")\n",
    "    train_iter = xs_gen()\n",
    "    test_iter = xs_gen(train=False)\n",
    "    # print('test_iter的值为',test_iter)\n",
    "    train, label = random_gen()\n",
    "    # print('label的值为', label)\n",
    "    testtrain, testlabel = random_gen(train=False)\n",
    "    # print('testlabel的值为', testlabel)\n",
    "\n",
    "    # 达到降维的目的，从原有的32767降到128维\n",
    "    model = tf.keras.models.load_model(filepath=r\"D:\\dianchi\\pilian\\zongfinishModel.h5\")\n",
    "    visualization_model = Model(inputs=model.input, outputs=model.get_layer(index=3).output)\n",
    "    pres = visualization_model.predict_generator(generator=train_iter, steps=math.ceil(Lens / Batch_size), verbose=1)\n",
    "    # ohpres = np.argmax(pres, axis=1)\n",
    "    # print(pres)\n",
    "    models = RandomForestClassifier(n_estimators=25,\n",
    "                                    n_jobs=2, min_samples_split=2, random_state=1, )\n",
    "    #\n",
    "    # # Fit on training data\n",
    "    models.fit(pres, label)\n",
    "    # print(pres.shape)\n",
    "\n",
    "    # Pre = models.predict(pres)\n",
    "    # num = 0\n",
    "    # for n in range(len(label)):\n",
    "    #     if int(Pre[n])== int(label[n] ):\n",
    "    #         num = num + 1\n",
    "    # print(\"准确率：\", num / len(label))\n",
    "\n",
    "    test_pres = visualization_model.predict_generator(generator=test_iter, steps=math.ceil(TestLens / Batch_size),\n",
    "                                                      verbose=1)\n",
    "    Pre = models.predict(test_pres)\n",
    "    print(Pre)\n",
    "    print(testlabel)\n",
    "    #\n",
    "    num = 0\n",
    "    # for n in range(len(testlabel)):\n",
    "    for n in range(len(testlabel) - 1):\n",
    "        if int(Pre[n]) == int(testlabel[n]):\n",
    "            num = num + 1\n",
    "    print(\"准确率：\", num / len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f10512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
